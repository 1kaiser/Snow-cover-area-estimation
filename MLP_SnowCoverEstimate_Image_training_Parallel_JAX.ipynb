{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/Snow-cover-area-estimation/blob/main/MLP_SnowCoverEstimate_Image_training_Parallel_JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "new test"
      ],
      "metadata": {
        "id": "SpFf0lyS7owY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "test for compatibility"
      ],
      "metadata": {
        "id": "VTLf6JluJKXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load DEM file (digital elevation map)\n",
        "!wget https://github.com/1kaiser/R_e/releases/download/1/x1_Dem.tif\n",
        "\n",
        "# load rainfall data 2016\n",
        "!wget https://github.com/1kaiser/R_e/releases/download/1/x1fileout_2016_HourlyRain.zip\n",
        "!unzip /content/x1fileout_2016_HourlyRain.zip\n",
        "rain_fplder = f'/content/rain'\n",
        "!mkdir -p {rain_fplder}\n",
        "!mv /content/*.txt {rain_fplder}\n",
        "\n",
        "# load soil map  data 2016\n",
        "!wget https://github.com/1kaiser/R_e/releases/download/1/x12016_soil_m.zip\n",
        "!unzip /content/x12016_soil_m.zip\n",
        "\n",
        "# load catchment/watershed shape file\n",
        "!wget https://github.com/1kaiser/R_e/releases/download/1/x1boundaryOutputx.zip\n",
        "!unzip /content/x1boundaryOutputx.zip"
      ],
      "metadata": {
        "id": "Ijro7Mb4LSRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing gdal binary\n",
        "!sudo apt-get install gdal-bin"
      ],
      "metadata": {
        "id": "vM-tUH7tPNx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from osgeo import gdal, osr\n",
        "from scipy.interpolate import griddata\n",
        "import multiprocessing\n",
        "\n",
        "# Specify the folder containing the data files\n",
        "folder_path_txt = '/content/rain'  # Update this to the actual folder path\n",
        "out_folder_path = '/content/rain_raster'\n",
        "shapefile = '/content/boundaryOutputx.shp'  # Update this to the actual shapefile path\n",
        "\n",
        "# Function to process a single data file\n",
        "def process_data_file(args):\n",
        "    input_filename, resolution = args  # Unpack the tuple\n",
        "\n",
        "    # Step 1: Read data from the text file without skipping any rows\n",
        "    data = np.genfromtxt(input_filename, delimiter=',', dtype=float)\n",
        "\n",
        "    # Extract columns from the data\n",
        "    latitudes = data[:, 0]\n",
        "    longitudes = data[:, 1]\n",
        "    rainfall = data[:, 6]\n",
        "\n",
        "    # Step 2: Calculate the number of columns and rows based on latitude and longitude extent\n",
        "    min_lat, max_lat = min(latitudes), max(latitudes)\n",
        "    min_lon, max_lon = min(longitudes), max(longitudes)\n",
        "    num_columns = int((max_lon - min_lon) / resolution) + 1\n",
        "    num_rows = int((max_lat - min_lat) / resolution) + 1\n",
        "\n",
        "    # Step 3: Interpolate the rainfall values onto the grid\n",
        "    grid_x, grid_y = np.meshgrid(np.linspace(min_lon, max_lon, num_columns), np.linspace(min_lat, max_lat, num_rows))\n",
        "    grid_z = griddata((longitudes, latitudes), rainfall, (grid_x, grid_y), method='cubic', fill_value=0)\n",
        "\n",
        "    # Step 4: Create a GeoTIFF raster using GDAL\n",
        "    driver = gdal.GetDriverByName('GTiff')\n",
        "    output_filename = os.path.join(folder_path_txt, f'{os.path.basename(input_filename)[:-4]}.tif')  # Same name as the original file\n",
        "    output_raster = driver.Create(output_filename, num_columns, num_rows, 1, gdal.GDT_Float32)\n",
        "    geotransform = (min_lon, resolution, 0, max_lat, 0, -resolution)\n",
        "    output_raster.SetGeoTransform(geotransform)\n",
        "    srs = osr.SpatialReference()\n",
        "    srs.ImportFromEPSG(4326)\n",
        "    output_raster.SetProjection(srs.ExportToWkt())\n",
        "    output_band = output_raster.GetRasterBand(1)\n",
        "    output_band.WriteArray(grid_z)\n",
        "    output_band.FlushCache()\n",
        "\n",
        "    output_raster = None  # Close the output raster file\n",
        "\n",
        "    # Step 5: Clip the rainfall raster using the shapefile\n",
        "    clip_output_raster = os.path.join(out_folder_path, f'{os.path.basename(input_filename)[:-4]}.tif')\n",
        "    temp_filename = os.path.join(out_folder_path, f't_{os.path.basename(input_filename)[:-4]}.tif')\n",
        "        # Open the TIFF file\n",
        "    !gdalwarp -tr 0.001 0.001 -overwrite {output_filename} {temp_filename}\n",
        "    !gdalwarp -cutline {shapefile} -dstnodata 0 -overwrite {temp_filename} {clip_output_raster}\n",
        "    !rm -rf {temp_filename}\n",
        "\n",
        "# Function to process data files in parallel\n",
        "def process_data_files_in_parallel(files, resolution=0.001):\n",
        "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())  # Use all available CPU cores\n",
        "    # Update the way arguments are passed to the process_data_file function\n",
        "    pool.map(process_data_file, [(file, resolution) for file in files])\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "# Get a list of all text files in the folder\n",
        "data_files = [os.path.join(folder_path_txt, filename) for filename in os.listdir(folder_path_txt) if filename.endswith(\".txt\")]\n",
        "\n",
        "\n",
        "!mkdir -p /content/rain_raster"
      ],
      "metadata": {
        "id": "6TnxSmYxLC7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/rain_raster/*"
      ],
      "metadata": {
        "id": "G0VsiXECUSWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to process data files in parallel with adjustable resolution\n",
        "process_data_files_in_parallel(data_files[1:20], resolution=0.001)  # Adjust the resolution as needed"
      ],
      "metadata": {
        "id": "09cd5ZeALSJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the folder containing the files\n",
        "folder_path = '/content/rain_raster'  # Update this to the actual folder path\n",
        "\n",
        "# Get a list of all files in the folder\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "# Iterate through the files and rename them\n",
        "for filename in files:\n",
        "    if filename.endswith(\".tif\"):\n",
        "        parts = filename.split(\"_\")  # Split the filename by underscores\n",
        "        if len(parts) == 5:\n",
        "            year = parts[1]\n",
        "            doy = int(parts[2])\n",
        "            hour = int(parts[3].split(\".\")[0])  # Remove the \".tif\" extension\n",
        "            new_filename = f\"{year}{doy:03d}{hour:02d}.tif\"\n",
        "            old_path = os.path.join(folder_path, filename)\n",
        "            new_path = os.path.join(folder_path, new_filename)\n",
        "            os.rename(old_path, new_path)\n",
        "            print(f\"Renamed {filename} to {new_filename}\")\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# Directory containing the files\n",
        "directory = '/content/2016_soil_m'\n",
        "\n",
        "# Automatically list all .tif files in the directory\n",
        "files = [f for f in os.listdir(directory) if f.endswith('.tif')]\n",
        "\n",
        "for file in files:\n",
        "    try:\n",
        "        # Extract yyyy, doy, and hh from the file name\n",
        "        parts = file.split('_')\n",
        "        yyyy_doy = parts[6][3:]  # Extract 'yyyydoy' from 'doyyyyydoy'\n",
        "        hh_segment = parts[5]  # Segment containing the hour 'sm_surface_7'\n",
        "        hh = ((int(hh_segment) + 1) * 3)  # Extract and calculate 'hh'\n",
        "\n",
        "        # Construct new file name\n",
        "        new_name = f\"{yyyy_doy}{hh:02d}.tif\"\n",
        "\n",
        "        # Full path for old and new file names\n",
        "        old_file = os.path.join(directory, file)\n",
        "        new_file = os.path.join(directory, new_name)\n",
        "\n",
        "        # Rename the file\n",
        "        os.rename(old_file, new_file)\n",
        "\n",
        "        print(f\"Renamed '{file}' to '{new_name}'\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Error processing file '{file}': {e}\")\n"
      ],
      "metadata": {
        "id": "itoJZkh_hzgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV data and extract DOY and discharge values\n",
        "def load_target_discharge_values(csv_file):\n",
        "    try:\n",
        "        # Load the CSV data\n",
        "        df = pd.read_csv(csv_file, header=None)\n",
        "\n",
        "        # Convert the first three columns to a single date column\n",
        "        df['Date'] = pd.to_datetime(df[[0, 1, 2]].astype(str).agg('-'.join, axis=1))\n",
        "\n",
        "        # Extract the DOY (day of year) and discharge values\n",
        "        df['DOY'] = df['Date'].dt.dayofyear\n",
        "        discharge_values = df[3].values\n",
        "\n",
        "        return discharge_values\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to load target discharge values: {str(e)}\")\n",
        "\n",
        "# Example usage:\n",
        "discharge_values = load_target_discharge_values('/content/test.csv')\n"
      ],
      "metadata": {
        "id": "TZYvEnqf0jLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from osgeo import gdal\n",
        "import glob\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Positional Encoding function\n",
        "def positional_encoding(inputs):\n",
        "    positional_encoding_dims = 6\n",
        "    image_height_x_image_width, cha = inputs.shape\n",
        "    inputs_freq = tf.stack([inputs * 2.0 ** i for i in range(positional_encoding_dims)])\n",
        "    x = tf.concat([tf.sin(inputs_freq), tf.cos(inputs_freq)], axis=-1)\n",
        "    x = tf.reshape(x, [image_height_x_image_width, -1])\n",
        "    x = tf.concat([inputs, x], axis=-1)\n",
        "    return x\n",
        "\n",
        "# Batch Encoding function\n",
        "def batch_encoded(inputs):\n",
        "    img_list = []\n",
        "    for i in range(inputs.shape[0]):\n",
        "        c = tf.reshape(inputs[i], [-1, inputs.shape[-1]])\n",
        "        p = positional_encoding(c)\n",
        "        img_list.append(tf.reshape(p, [inputs.shape[1], inputs.shape[2], p.shape[-1]]))\n",
        "    return tf.stack(img_list)\n",
        "\n",
        "# Function to load GDAL raster data\n",
        "def load_raster_data(elevation_file, soil_moisture_file, rainfall_file):\n",
        "    elevation_ds = gdal.Open(elevation_file)\n",
        "    soil_moisture_ds = gdal.Open(soil_moisture_file)\n",
        "    rainfall_ds = gdal.Open(rainfall_file)\n",
        "\n",
        "    if elevation_ds is None or soil_moisture_ds is None or rainfall_ds is None:\n",
        "        raise Exception(\"Failed to open one or more GDAL raster files.\")\n",
        "\n",
        "    elevation_data = elevation_ds.ReadAsArray()\n",
        "    soil_moisture_data = soil_moisture_ds.ReadAsArray()\n",
        "    rainfall_data = rainfall_ds.ReadAsArray()\n",
        "\n",
        "    return elevation_data, soil_moisture_data, rainfall_data\n",
        "\n",
        "# Function to load target discharge values from CSV\n",
        "def load_target_discharge_values(csv_file):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file, header=None)\n",
        "        df['Date'] = pd.to_datetime(df[[0, 1, 2]].astype(str).agg('-'.join, axis=1))\n",
        "        df['DOY'] = df['Date'].dt.dayofyear\n",
        "        discharge_values = df[3].values\n",
        "        return discharge_values\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to load target discharge values: {str(e)}\")\n",
        "\n",
        "# Create and compile the model for color to grayscale image conversion\n",
        "def create_model(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    flat_input = tf.keras.layers.Flatten()(input_layer)\n",
        "\n",
        "    ndl = 8\n",
        "    dlw = 256\n",
        "\n",
        "    x = flat_input\n",
        "\n",
        "    for i in range(ndl):\n",
        "        x = Dense(dlw, activation='relu')(x)\n",
        "\n",
        "        if i == 4:\n",
        "            x = Concatenate()([x, flat_input])\n",
        "\n",
        "    output_layer = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train the model with batched input files\n",
        "def train_model_with_batch(input_files, target_discharge_data, num_files_per_batch, epochs):\n",
        "    for i in range(0, len(input_files), num_files_per_batch):\n",
        "        batch_input_files = input_files[i:i+num_files_per_batch]\n",
        "        batch_target_discharge_data = target_discharge_data[i:i+num_files_per_batch]\n",
        "\n",
        "        batch_input_data = []\n",
        "        for elevation_file, rainfall_file, soil_moisture_file in batch_input_files:\n",
        "            elevation_data, soil_moisture_data, rainfall_data = load_raster_data(elevation_file, soil_moisture_file, rainfall_file)\n",
        "            input_data = np.stack([rainfall_data, soil_moisture_data, elevation_data], axis=-1)\n",
        "            input_data = batch_encoded(input_data)  # Apply batch encoding\n",
        "            batch_input_data.append(input_data)\n",
        "\n",
        "        batch_input_data = np.array(batch_input_data)\n",
        "\n",
        "        model.fit(batch_input_data, batch_target_discharge_data, epochs=epochs)\n",
        "\n",
        "# Define the paths to the common elevation file and folders\n",
        "elevation_file = '/content/x1_Dem.tif'  # Replace with the actual path to the elevation raster file\n",
        "rainfall_folder = '/content/rain_raster'  # Replace with the path to the rainfall folder\n",
        "soil_moisture_folder = '/content/2016_soil_m'  # Replace with the path to the soil moisture folder\n",
        "csv_file = '/content/test.csv'  # Replace with the path to the target discharge CSV file\n",
        "\n",
        "# Load target discharge data from CSV\n",
        "discharge_values = load_target_discharge_values(csv_file)\n",
        "\n",
        "# List all the files in the rainfall and soil moisture folders\n",
        "rainfall_files = glob.glob(os.path.join(rainfall_folder, '*.tif'))\n",
        "soil_moisture_files = glob.glob(os.path.join(soil_moisture_folder, '*.tif'))\n",
        "\n",
        "# Create a list of input file tuples with the common elevation file\n",
        "input_files = [(elevation_file, rain_file, soil_moisture_file) for rain_file, soil_moisture_file in zip(rainfall_files, soil_moisture_files)]\n",
        "\n",
        "# Define the input shape based on your data dimensions\n",
        "input_shape = load_raster_data(elevation_file, soil_moisture_files[0], rainfall_files[0])[0].shape + (3,)\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_model(input_shape)\n",
        "\n",
        "# Train the model with your data\n",
        "train_model_with_batch(input_files, discharge_values, num_files_per_batch=10, epochs=5)\n"
      ],
      "metadata": {
        "id": "WbigqOngbShp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "x test"
      ],
      "metadata": {
        "id": "cYwHOTb-jm4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "from osgeo import gdal\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from osgeo import gdal, osr\n",
        "\n",
        "def reproject_raster(input_raster, match_raster, output_raster):\n",
        "    src_ds = gdal.Open(input_raster)\n",
        "    match_ds = gdal.Open(match_raster)\n",
        "\n",
        "    match_proj = match_ds.GetProjection()\n",
        "    match_geotrans = match_ds.GetGeoTransform()\n",
        "    match_band = match_ds.GetRasterBand(1)\n",
        "    x_size = match_band.XSize\n",
        "    y_size = match_band.YSize\n",
        "\n",
        "    driver = gdal.GetDriverByName('GTiff')\n",
        "    dst_ds = driver.Create(output_raster, x_size, y_size, 1, match_band.DataType)\n",
        "    dst_ds.SetGeoTransform(match_geotrans)\n",
        "    dst_ds.SetProjection(match_proj)\n",
        "\n",
        "    gdal.ReprojectImage(src_ds, dst_ds, src_ds.GetProjection(), match_proj, gdal.GRA_NearestNeighbour)\n",
        "\n",
        "    src_ds = None\n",
        "    match_ds = None\n",
        "    dst_ds = None\n",
        "\n",
        "def load_raster_data(elevation_file, soil_moisture_file, rainfall_file):\n",
        "    # Reproject soil moisture and rainfall rasters\n",
        "    reprojected_soil_moisture = '/content/reprojected_soil_moisture.tif'\n",
        "    reprojected_rainfall = '/content/reprojected_rainfall.tif'\n",
        "\n",
        "    reproject_raster(soil_moisture_file, elevation_file, reprojected_soil_moisture)\n",
        "    reproject_raster(rainfall_file, elevation_file, reprojected_rainfall)\n",
        "\n",
        "    # Open the reprojected and elevation raster datasets\n",
        "    elevation_ds = gdal.Open(elevation_file)\n",
        "    soil_moisture_ds = gdal.Open(reprojected_soil_moisture)\n",
        "    rainfall_ds = gdal.Open(reprojected_rainfall)\n",
        "\n",
        "    if not elevation_ds or not soil_moisture_ds or not rainfall_ds:\n",
        "        raise IOError(f\"Failed to open one or more files: {elevation_file}, {reprojected_soil_moisture}, {reprojected_rainfall}\")\n",
        "\n",
        "    elevation_data = elevation_ds.ReadAsArray()\n",
        "    soil_moisture_data = soil_moisture_ds.ReadAsArray()\n",
        "    rainfall_data = rainfall_ds.ReadAsArray()\n",
        "\n",
        "    return elevation_data, soil_moisture_data, rainfall_data\n",
        "\n",
        "\n",
        "# Function to load target discharge values from a CSV file\n",
        "def load_target_discharge_values(csv_file):\n",
        "    df = pd.read_csv(csv_file, header=None)\n",
        "    df['Date'] = pd.to_datetime(df[[0, 1, 2]].astype(str).agg('-'.join, axis=1))\n",
        "    df['DOY'] = df['Date'].dt.dayofyear\n",
        "    discharge_values = df[3].values\n",
        "    return discharge_values\n",
        "\n",
        "# Positional Encoding function\n",
        "def positional_encoding(inputs):\n",
        "    positional_encoding_dims = 6\n",
        "    inputs_freq = tf.stack([inputs * 2.0 ** i for i in range(positional_encoding_dims)])\n",
        "    x = tf.concat([tf.sin(inputs_freq), tf.cos(inputs_freq)], axis=-1)\n",
        "    x = tf.reshape(x, [inputs.shape[0], -1])\n",
        "    x = tf.concat([inputs, x], axis=-1)\n",
        "    return x\n",
        "\n",
        "# Function to apply batch encoding\n",
        "def batch_encoded(inputs):\n",
        "    img_list = []\n",
        "    for i in range(inputs.shape[0]):\n",
        "        c = tf.reshape(inputs[i], [-1, inputs.shape[-1]])\n",
        "        p = positional_encoding(c)\n",
        "        img_list.append(tf.reshape(p, [inputs.shape[1], inputs.shape[2], p.shape[-1]]))\n",
        "    return tf.stack(img_list)\n",
        "\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Reduced number of filters in Conv2D layers\n",
        "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Reduced number of neurons in Dense layer\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "\n",
        "# Function to generate slice bounding boxes\n",
        "def get_slice_bboxes(image_height, image_width, slice_height=224, slice_width=224, overlap_height_ratio=0.0, overlap_width_ratio=0.0):\n",
        "    slice_bboxes = []\n",
        "    y_overlap = int(overlap_height_ratio * slice_height)\n",
        "    x_overlap = int(overlap_width_ratio * slice_width)\n",
        "    y_max = 0\n",
        "\n",
        "    while y_max < image_height:\n",
        "        x_min = 0\n",
        "        y_max = min(y_max + slice_height, image_height)\n",
        "        while x_min < image_width:\n",
        "            x_max = min(x_min + slice_width, image_width)\n",
        "            slice_bboxes.append([x_min, y_max - slice_height, x_max, y_max])\n",
        "            x_min = x_max - x_overlap\n",
        "        y_max = y_max - y_overlap\n",
        "\n",
        "    return slice_bboxes\n",
        "\n",
        "# Function to slice image data\n",
        "def slicedImageInput(bbox, batch):\n",
        "    x_images = batch[:, bbox[1]:bbox[3], bbox[0]:bbox[2], :]\n",
        "    return x_images\n",
        "\n",
        "\n",
        "import gc  # Garbage Collector\n",
        "\n",
        "def data_generator(file_tuples, batch_size, target_discharge_data, slice_bboxes, slice_batch_size=10):\n",
        "    for i in range(0, len(file_tuples), batch_size):\n",
        "        batch_files = file_tuples[i:i + batch_size]\n",
        "        batch_target_data = target_discharge_data[i:i + batch_size]\n",
        "\n",
        "        for elevation_file, soil_moisture_file, rainfall_file in batch_files:\n",
        "            elevation_data, soil_moisture_data, rainfall_data = load_raster_data(elevation_file, soil_moisture_file, rainfall_file)\n",
        "            input_data = np.stack([rainfall_data, soil_moisture_data, elevation_data], axis=-1)\n",
        "            input_data = batch_encoded(input_data)\n",
        "\n",
        "            # Process slices in smaller batches\n",
        "            for j in range(0, len(slice_bboxes), slice_batch_size):\n",
        "                slice_batch = slice_bboxes[j:j + slice_batch_size]\n",
        "                batch_input_data = []\n",
        "\n",
        "                for bbox in slice_batch:\n",
        "                    sliced_input = slicedImageInput(bbox, input_data)\n",
        "                    batch_input_data.append(sliced_input)\n",
        "\n",
        "                yield np.concatenate(batch_input_data), np.array(batch_target_data)\n",
        "\n",
        "                del batch_input_data  # Free memory\n",
        "                gc.collect()  # Call garbage collector\n",
        "\n",
        "\n",
        "# Function to train the model with batched and sliced input files\n",
        "def train_model_with_batch(model, input_files, target_discharge_data, batch_size, epochs, slice_bboxes):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        gen = data_generator(input_files, batch_size, target_discharge_data, slice_bboxes)\n",
        "        for batch_input, batch_target in gen:\n",
        "            model.fit(batch_input, batch_target, epochs=1, verbose=1)\n",
        "\n",
        "# Main processing setup\n",
        "elevation_file = '/content/x1_Dem.tif'\n",
        "rainfall_folder = '/content/rain_raster'\n",
        "soil_moisture_folder = '/content/2016_soil_m'\n",
        "csv_file = '/content/test.csv'\n",
        "\n",
        "discharge_values = load_target_discharge_values(csv_file)\n",
        "\n",
        "rainfall_files = glob.glob(os.path.join(rainfall_folder, '*.tif'))\n",
        "soil_moisture_files = glob.glob(os.path.join(soil_moisture_folder, '*.tif'))\n",
        "input_files = [(elevation_file, soil_moisture_file, rainfall_file) for soil_moisture_file, rainfall_file in zip(soil_moisture_files, rainfall_files)]\n",
        "\n",
        "sample_elevation, sample_soil, sample_rain = load_raster_data(elevation_file, soil_moisture_files[0], rainfall_files[0])\n",
        "input_shape = (sample_elevation.shape[0], sample_elevation.shape[1], 3)\n",
        "\n",
        "model = create_model(input_shape)\n",
        "\n",
        "# Define slice dimensions and overlap\n",
        "slice_bboxes = get_slice_bboxes(sample_elevation.shape[0], sample_elevation.shape[1], slice_height=224, slice_width=224, overlap_height_ratio=0.1, overlap_width_ratio=0.1)\n",
        "\n",
        "# Train the model\n",
        "train_model_with_batch(model, input_files, discharge_values, batch_size=1, epochs=5, slice_bboxes=slice_bboxes)\n"
      ],
      "metadata": {
        "id": "FeStbcjrjoAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "part2"
      ],
      "metadata": {
        "id": "ve6-W3fj3pXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "from osgeo import gdal\n",
        "\n",
        "from osgeo import gdal\n",
        "\n",
        "def load_and_reproject_raster(elevation_file, other_file):\n",
        "    # Open the elevation and the other raster datasets\n",
        "    elevation_ds = gdal.Open(elevation_file)\n",
        "    other_ds = gdal.Open(other_file)\n",
        "\n",
        "    if not elevation_ds or not other_ds:\n",
        "        raise IOError(\"Failed to open one or more of the raster files.\")\n",
        "\n",
        "    # Get projection and geotransform from the elevation dataset\n",
        "    match_proj = elevation_ds.GetProjection()\n",
        "    match_geotrans = elevation_ds.GetGeoTransform()\n",
        "    match_band = elevation_ds.GetRasterBand(1)\n",
        "    x_size, y_size = match_band.XSize, match_band.YSize\n",
        "\n",
        "    # Create a temporary dataset for reprojection\n",
        "    temp_filename = '/content/temp.tif'  # Replace with a valid path\n",
        "    driver = gdal.GetDriverByName('GTiff')\n",
        "    temp_ds = driver.Create(temp_filename, x_size, y_size, 1, match_band.DataType)\n",
        "    if not temp_ds:\n",
        "        raise Exception(\"Failed to create temporary dataset.\")\n",
        "\n",
        "    # Set geotransform and projection to the temporary dataset\n",
        "    temp_ds.SetGeoTransform(match_geotrans)\n",
        "    temp_ds.SetProjection(match_proj)\n",
        "\n",
        "    # Reproject the other raster to match the elevation raster\n",
        "    gdal.ReprojectImage(other_ds, temp_ds, other_ds.GetProjection(), match_proj, gdal.GRA_NearestNeighbour)\n",
        "\n",
        "    # Read the reprojected data as arrays\n",
        "    other_data = temp_ds.GetRasterBand(1).ReadAsArray()\n",
        "    elevation_data = elevation_ds.GetRasterBand(1).ReadAsArray()\n",
        "\n",
        "    # Close datasets\n",
        "    elevation_ds = None\n",
        "    other_ds = None\n",
        "    temp_ds = None\n",
        "\n",
        "    return other_data, elevation_data\n",
        "\n",
        "\n",
        "def load_target_discharge_values(csv_file):\n",
        "    df = pd.read_csv(csv_file, usecols=[3])\n",
        "    return df.squeeze(\"columns\").values\n",
        "\n",
        "def create_model(input_shape):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "def data_generator(file_tuples, target_discharge_data, batch_size=1):\n",
        "    for i in range(0, len(file_tuples), batch_size):\n",
        "        batch_files = file_tuples[i:i + batch_size]\n",
        "        batch_target_data = target_discharge_data[i:i + batch_size]\n",
        "\n",
        "        batch_input_data = []\n",
        "        for elevation_file, other_file in batch_files:\n",
        "            soil_moisture_data, elevation_data = load_and_reproject_raster(elevation_file, other_file)\n",
        "            batch_input_data.append(np.dstack([soil_moisture_data, elevation_data]))\n",
        "\n",
        "        yield np.stack(batch_input_data), np.array(batch_target_data)\n",
        "\n",
        "elevation_file = '/content/x1_Dem.tif'\n",
        "soil_moisture_folder = '/content/2016_soil_m'\n",
        "csv_file = '/content/test.csv'\n",
        "\n",
        "discharge_values = load_target_discharge_values(csv_file)\n",
        "\n",
        "import os\n",
        "soil_moisture_files = glob.glob(os.path.join(soil_moisture_folder, '*.tif'))\n",
        "input_files = [(elevation_file, soil_moisture_file) for soil_moisture_file in soil_moisture_files]\n",
        "\n",
        "sample_soil, sample_elevation = load_and_reproject_raster(elevation_file, soil_moisture_files[0])\n",
        "input_shape = (sample_elevation.shape[0], sample_elevation.shape[1], 2)\n",
        "\n",
        "model = create_model(input_shape)\n",
        "gen = data_generator(input_files, discharge_values)\n",
        "model.fit(gen, epochs=5, steps_per_epoch=len(input_files))\n"
      ],
      "metadata": {
        "id": "fn2OayYz3pBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "from osgeo import gdal\n",
        "\n",
        "def load_and_reproject_raster(elevation_file, soil_moisture_file, rainfall_file):\n",
        "    # Open the elevation, soil moisture, and rainfall raster datasets\n",
        "    elevation_ds = gdal.Open(elevation_file)\n",
        "    soil_moisture_ds = gdal.Open(soil_moisture_file)\n",
        "    rainfall_ds = gdal.Open(rainfall_file)\n",
        "\n",
        "    if not elevation_ds or not soil_moisture_ds or not rainfall_ds:\n",
        "        raise IOError(\"Failed to open one or more of the raster files.\")\n",
        "\n",
        "    # Get projection and geotransform from the elevation dataset\n",
        "    match_proj = elevation_ds.GetProjection()\n",
        "    match_geotrans = elevation_ds.GetGeoTransform()\n",
        "    match_band = elevation_ds.GetRasterBand(1)\n",
        "    x_size, y_size = match_band.XSize, match_band.YSize\n",
        "\n",
        "    # Create temporary datasets for reprojection\n",
        "    temp_elevation_ds = gdal.GetDriverByName('MEM').Create('', x_size, y_size, 1, match_band.DataType)\n",
        "    temp_soil_moisture_ds = gdal.GetDriverByName('MEM').Create('', x_size, y_size, 1, match_band.DataType)\n",
        "    temp_rainfall_ds = gdal.GetDriverByName('MEM').Create('', x_size, y_size, 1, match_band.DataType)\n",
        "\n",
        "    if not temp_elevation_ds or not temp_soil_moisture_ds or not temp_rainfall_ds:\n",
        "        raise Exception(\"Failed to create temporary datasets.\")\n",
        "\n",
        "    # Set geotransform and projection to the temporary datasets\n",
        "    temp_elevation_ds.SetGeoTransform(match_geotrans)\n",
        "    temp_elevation_ds.SetProjection(match_proj)\n",
        "    temp_soil_moisture_ds.SetGeoTransform(match_geotrans)\n",
        "    temp_soil_moisture_ds.SetProjection(match_proj)\n",
        "    temp_rainfall_ds.SetGeoTransform(match_geotrans)\n",
        "    temp_rainfall_ds.SetProjection(match_proj)\n",
        "\n",
        "    # Reproject the soil moisture raster to match the elevation raster\n",
        "    gdal.ReprojectImage(soil_moisture_ds, temp_soil_moisture_ds, soil_moisture_ds.GetProjection(), match_proj, gdal.GRA_NearestNeighbour)\n",
        "\n",
        "    # Reproject the rainfall raster to match the elevation raster\n",
        "    gdal.ReprojectImage(rainfall_ds, temp_rainfall_ds, rainfall_ds.GetProjection(), match_proj, gdal.GRA_NearestNeighbour)\n",
        "\n",
        "    # Read the reprojected data as arrays\n",
        "    soil_moisture_data = temp_soil_moisture_ds.GetRasterBand(1).ReadAsArray()\n",
        "    elevation_data = elevation_ds.GetRasterBand(1).ReadAsArray()\n",
        "    rainfall_data = temp_rainfall_ds.GetRasterBand(1).ReadAsArray()\n",
        "\n",
        "    # Close datasets\n",
        "    elevation_ds = None\n",
        "    soil_moisture_ds = None\n",
        "    rainfall_ds = None\n",
        "    temp_elevation_ds = None\n",
        "    temp_soil_moisture_ds = None\n",
        "    temp_rainfall_ds = None\n",
        "\n",
        "    return soil_moisture_data, elevation_data, rainfall_data\n",
        "\n",
        "\n",
        "\n",
        "def load_target_discharge_values(csv_file):\n",
        "    df = pd.read_csv(csv_file, usecols=[3])\n",
        "    return df.squeeze(\"columns\").values\n",
        "\n",
        "def create_model(input_shape):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "def data_generator(file_tuples, target_discharge_data, batch_size=1):\n",
        "    for i in range(0, len(file_tuples), batch_size):\n",
        "        batch_files = file_tuples[i:i + batch_size]\n",
        "        batch_target_data = target_discharge_data[i:i + batch_size]\n",
        "\n",
        "        batch_input_data = []\n",
        "        for soil_moisture_file, rainfall_file in batch_files:\n",
        "            soil_moisture_data, elevation_data, rainfall_data = load_and_reproject_raster(elevation_file, soil_moisture_file, rainfall_file)\n",
        "            batch_input_data.append(np.dstack([soil_moisture_data, elevation_data, rainfall_data]))\n",
        "\n",
        "        yield np.stack(batch_input_data), np.array(batch_target_data)\n",
        "\n",
        "elevation_file = '/content/x1_Dem.tif'\n",
        "soil_moisture_folder = '/content/2016_soil_m'\n",
        "csv_file = '/content/test.csv'\n",
        "rainfall_folder = '/content/rain_raster'\n",
        "discharge_values = load_target_discharge_values(csv_file)\n",
        "\n",
        "# List of soil moisture and rainfall files\n",
        "soil_moisture_files = sorted(glob.glob(os.path.join(soil_moisture_folder, '*.tif')))\n",
        "rainfall_files = sorted(glob.glob(os.path.join(rainfall_folder, '*.tif')))\n",
        "\n",
        "# Create input file tuples with elevation, soil moisture, and rainfall files\n",
        "input_files = [(soil_moisture_file, rainfall_file) for soil_moisture_file, rainfall_file in zip(soil_moisture_files, rainfall_files)]\n",
        "\n",
        "# Load a sample of soil moisture, elevation, and rainfall data to get input shape\n",
        "sample_soil, sample_elevation, sample_rainfall = load_and_reproject_raster(elevation_file, input_files[0][0], input_files[0][1])\n",
        "input_shape = (sample_elevation.shape[0], sample_elevation.shape[1], 3)\n",
        "\n",
        "# Create the model\n",
        "model = create_model(input_shape)\n",
        "\n",
        "# Create a data generator\n",
        "gen = data_generator(input_files, discharge_values)\n",
        "\n",
        "# Train the model\n",
        "model.fit(gen, epochs=5, steps_per_epoch=len(input_files))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_bTpTzUC5qU",
        "outputId": "60e48d0d-9d01-4d2e-ef5f-1867a66362ca"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "19/19 [==============================] - 31s 1s/step - loss: 266231231807488.0000\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 95 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r19/19 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f9c4556dfc0>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "file matching"
      ],
      "metadata": {
        "id": "-vHAmHXNWNGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Paths to the directories\n",
        "rainfall_folder = '/content/rain_raster'  # Update this path as needed\n",
        "soil_moisture_folder = '/content/2016_soil_m'  # Update this path as needed\n",
        "\n",
        "# List all files in each directory\n",
        "rainfall_files = {f for f in os.listdir(rainfall_folder) if f.endswith('.tif')}\n",
        "soil_moisture_files = {f for f in os.listdir(soil_moisture_folder) if f.endswith('.tif')}\n",
        "\n",
        "# Find matching files\n",
        "matching_files = rainfall_files.intersection(soil_moisture_files)\n",
        "\n",
        "# Display matching file pairs\n",
        "matching_pairs = [(os.path.join(rainfall_folder, f), os.path.join(soil_moisture_folder, f)) for f in matching_files]\n",
        "for pair in matching_pairs:\n",
        "    print(pair)\n"
      ],
      "metadata": {
        "id": "k3v-iO-Eku8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from osgeo import gdal\n",
        "\n",
        "def get_raster_shape(file_path):\n",
        "    dataset = gdal.Open(file_path)\n",
        "    if dataset is None:\n",
        "        return None\n",
        "    return (dataset.RasterYSize, dataset.RasterXSize)\n",
        "\n",
        "def list_files_in_directory(directory):\n",
        "    return [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.tif')]\n",
        "\n",
        "def get_shapes_of_tiffs_in_directory(directory):\n",
        "    files = list_files_in_directory(directory)\n",
        "    return {file: get_raster_shape(file) for file in files}\n",
        "\n",
        "# File paths\n",
        "elevation_file = '/content/x1_Dem.tif'\n",
        "rainfall_folder = '/content/rain_raster'\n",
        "soil_moisture_folder = '/content/2016_soil_m'\n",
        "\n",
        "# Execute the functions\n",
        "elevation_shape = get_raster_shape(elevation_file)\n",
        "rainfall_shapes = get_shapes_of_tiffs_in_directory(rainfall_folder)\n",
        "soil_moisture_shapes = get_shapes_of_tiffs_in_directory(soil_moisture_folder)\n",
        "\n",
        "# Print the results\n",
        "print(\"Elevation Shape:\", elevation_shape)\n",
        "print(\"Rainfall Shapes:\", rainfall_shapes)\n",
        "print(\"Soil Moisture Shapes:\", soil_moisture_shapes)\n"
      ],
      "metadata": {
        "id": "r8_Lj3K8nGdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mnist test"
      ],
      "metadata": {
        "id": "d-iMEGN-JHaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "apply_positional_encoding = True\n",
        "ndl = 8  # Number of dense layers in MLP\n",
        "dlw = 256  # Dense layer width\n",
        "\n",
        "# Positional Encoding function\n",
        "def positional_encoding(inputs):\n",
        "    positional_encoding_dims = 6\n",
        "    image_height_x_image_width, cha = inputs.shape\n",
        "    inputs_freq = tf.stack([inputs * 2.0 ** i for i in range(positional_encoding_dims)])\n",
        "    x = tf.concat([tf.sin(inputs_freq), tf.cos(inputs_freq)], axis=-1)\n",
        "    x = tf.reshape(x, [image_height_x_image_width, -1])\n",
        "    x = tf.concat([inputs, x], axis=-1)\n",
        "    return x\n",
        "\n",
        "# Batch Encoding function\n",
        "def batch_encoded(inputs):\n",
        "    img_list = []\n",
        "    for i in range(inputs.shape[0]):\n",
        "        c = tf.reshape(inputs[i], [-1, inputs.shape[-1]])\n",
        "        p = positional_encoding(c)\n",
        "        img_list.append(tf.reshape(p, [inputs.shape[1], inputs.shape[2], p.shape[-1]]))\n",
        "    return tf.stack(img_list)\n",
        "\n",
        "# Convert grayscale MNIST images to artificial color images\n",
        "def convert_to_color_mnist(grayscale_images):\n",
        "    return np.repeat(grayscale_images[..., np.newaxis], 3, axis=-1)\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Convert grayscale images to color\n",
        "color_mnist_train_images = convert_to_color_mnist(mnist_train_images)\n",
        "color_mnist_test_images = convert_to_color_mnist(mnist_test_images)\n",
        "\n",
        "# Normalize the color images to the range [0, 1]\n",
        "color_mnist_train_images = color_mnist_train_images.astype(\"float32\") / 255.0\n",
        "color_mnist_test_images = color_mnist_test_images.astype(\"float32\") / 255.0\n",
        "\n",
        "# Create and compile the model for color to grayscale image conversion\n",
        "def create_model():\n",
        "    input_layer = tf.keras.layers.Input(shape=(28, 28, 3))\n",
        "\n",
        "    x = input_layer\n",
        "\n",
        "    for i in range(ndl):\n",
        "        x = tf.keras.layers.Dense(dlw, activation='relu')(x)\n",
        "\n",
        "        # Add residual connection after the 4th dense layer\n",
        "        if i == 4:\n",
        "            x = tf.keras.layers.Concatenate()([x, input_layer])  # Residual connection\n",
        "\n",
        "    output_layer = tf.keras.layers.Dense(1)(x)  # Output is grayscale, so only 1 channel\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Reshape grayscale labels to match the model's output shape\n",
        "mnist_train_labels = mnist_train_labels.reshape(-1, 1, 1, 1)\n",
        "mnist_test_labels = mnist_test_labels.reshape(-1, 1, 1, 1)\n",
        "\n",
        "# Detect TPU and return appropriate distribution strategy\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.get_strategy()  # Default strategy that works on CPU and single GPU\n",
        "\n",
        "with strategy.scope():\n",
        "    model = create_model()\n",
        "\n",
        "# Train the model on color images and predict grayscale images\n",
        "model.fit(color_mnist_train_images, mnist_train_labels, epochs=5)\n",
        "predicted_grayscale_images = model.predict(color_mnist_test_images)\n"
      ],
      "metadata": {
        "id": "B6WnRp2bTlo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have a variable predicted_grayscale_images containing the predicted grayscale images\n",
        "\n",
        "# Plot the first five predicted grayscale images\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(5):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(predicted_grayscale_images[i, :, :, 0], cmap='gray')\n",
        "    plt.title(f\"Predicted Image {i + 1}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IknRfoEKUd_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**setting up the data pipeline**"
      ],
      "metadata": {
        "id": "_IroJNv2LglP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "dir='/content/files'\n",
        "urls = 'https://github.com/1kaiser/Snow-cover-area-estimation/releases/download/v1/imagesfolder.zip'\n",
        "data_dir = tf.keras.utils.get_file(origin=urls,\n",
        "                                   fname='s',\n",
        "                                   cache_subdir= dir,\n",
        "                                   archive_format='auto',\n",
        "                                   untar=False,\n",
        "                                   extract=True)\n",
        "!rm -r {dir}/s\n",
        "data_dir = pathlib.Path(data_dir)"
      ],
      "metadata": {
        "id": "jZMIymWpLoHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GeoTiff to Image & Image to Geotiff conversion\n"
      ],
      "metadata": {
        "id": "v_QdbhBZLoHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os.path\n",
        "import re\n",
        "\n",
        "from osgeo import gdal\n",
        "from osgeo import gdal_array\n",
        "from osgeo import osr\n",
        "\n",
        "def get_gain_band(input_file):\n",
        "    \"\"\"get GAIN_BAND from meta file (*.tif.txt)\"\"\"\n",
        "     # define file name of *.tif.txt\n",
        "    ifile_txt = re.sub(r'.tif', '.tif.txt', input_file)\n",
        "    ld = open(ifile_txt)\n",
        "    lines = ld.readlines()\n",
        "    ld.close()\n",
        "\n",
        "    gain_band = []\n",
        "    for line in lines:\n",
        "        if line.find(\"GAIN_BAND\") >= 0:\n",
        "             gain_band.append(float((re.split(' ', line)[1]).strip()))\n",
        "    return gain_band\n",
        "\n",
        "def tif2array(input_file, calc_gain=True):\n",
        "    \"\"\"\n",
        "    read GeoTiff and convert to numpy.ndarray.\n",
        "    Inputs:\n",
        "        input_file (str) : the name of input GeoTiff file.\n",
        "        calc_gain (bool) : wheter calc GAIN to DN  or not (defaul:True).\n",
        "    return:\n",
        "        image(np.array) : image for each bands\n",
        "        dataset : for gdal's data drive.\n",
        "    \"\"\"\n",
        "    dataset = gdal.Open(input_file, gdal.GA_ReadOnly)\n",
        "    # Allocate our array using the first band's datatype\n",
        "    image_datatype = dataset.GetRasterBand(1).DataType\n",
        "    image = np.zeros((dataset.RasterYSize, dataset.RasterXSize, dataset.RasterCount),\n",
        "                     dtype=float)\n",
        "\n",
        "    if calc_gain == True:\n",
        "        # get gain\n",
        "        gain = get_gain_band(input_file)\n",
        "\n",
        "    # Loop over all bands in dataset\n",
        "    for b in range(dataset.RasterCount):\n",
        "        # Remember, GDAL index is on 1, but Python is on 0 -- so we add 1 for our GDAL calls\n",
        "        band = dataset.GetRasterBand(b + 1)\n",
        "        # Read in the band's data into the third dimension of our array\n",
        "        if calc_gain == True:\n",
        "            # calc gain value for each bands\n",
        "            image[:, :, b] = band.ReadAsArray() * gain[b]\n",
        "        else:\n",
        "            image[:, :, b] = band.ReadAsArray()\n",
        "    return image, dataset\n",
        "\n",
        "def array2raster(newRasterfn, dataset, array, dtype):\n",
        "    \"\"\"\n",
        "    save GTiff file from numpy.array\n",
        "    input:\n",
        "        newRasterfn: save file name\n",
        "        dataset : original tif file\n",
        "        array : numpy.array\n",
        "        dtype: Byte or Float32.\n",
        "    \"\"\"\n",
        "    cols = array.shape[1]\n",
        "    rows = array.shape[0]\n",
        "    originX, pixelWidth, b, originY, d, pixelHeight = dataset.GetGeoTransform()\n",
        "\n",
        "    driver = gdal.GetDriverByName('GTiff')\n",
        "\n",
        "    # set data type to save.\n",
        "    GDT_dtype = gdal.GDT_Unknown\n",
        "    if dtype == \"Byte\":\n",
        "        GDT_dtype = gdal.GDT_Byte\n",
        "    elif dtype == \"Float32\":\n",
        "        GDT_dtype = gdal.GDT_Float32\n",
        "    else:\n",
        "        print(\"Not supported data type.\")\n",
        "\n",
        "    # set number of band.\n",
        "    if array.ndim == 2:\n",
        "        band_num = 1\n",
        "    else:\n",
        "        band_num = array.shape[2]\n",
        "\n",
        "    outRaster = driver.Create(newRasterfn, cols, rows, band_num, GDT_dtype)\n",
        "    outRaster.SetGeoTransform((originX, pixelWidth, 0, originY, 0, pixelHeight))\n",
        "\n",
        "    # # Loop over all bands.\n",
        "    # for b in range(band_num):\n",
        "    #     outband = outRaster.GetRasterBand(b + 1)\n",
        "    #     # Read in the band's data into the third dimension of our array\n",
        "    #     if band_num == 1:\n",
        "    #         outband.WriteArray(array)\n",
        "    #     else:\n",
        "    #         outband.WriteArray(array[:,:,b])\n",
        "    outband = outRaster.GetRasterBand(1)\n",
        "    outband.WriteArray(array.reshape(rows, cols))\n",
        "    # setting srs from input tif file.\n",
        "    prj=dataset.GetProjection()\n",
        "    outRasterSRS = osr.SpatialReference(wkt=prj)\n",
        "    outRaster.SetProjection(outRasterSRS.ExportToWkt())\n",
        "    outband.FlushCache()\n",
        "    return newRasterfn"
      ],
      "metadata": {
        "id": "i13qZ8GyLoHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = r'/content/files/'\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "end = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b1 = prefix+end[0]\n",
        "expression_b2 = prefix+end[1]\n",
        "expression_b3 = prefix+end[2]\n",
        "expression_b4 = prefix+end[3]\n",
        "expression_b5 = prefix+end[4]\n",
        "expression_b6 = prefix+end[5]\n",
        "expression_b7 = prefix+end[6]\n",
        "\n",
        "\n",
        "imgs_list_b1 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b1)]\n",
        "\n",
        "imgs_list_b1.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n"
      ],
      "metadata": {
        "id": "-Vlw7LYCLoHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "temp_dir = r'/content/'\n",
        "def ybatchedimages(images_path, image_list, batch_idx):\n",
        "  images = []\n",
        "  for id in range(len(batch_idx)):\n",
        "    path = os.path.join(images_path, image_list[id])\n",
        "    pathb2 = path.replace(expression_b1, expression_b2)\n",
        "    pathb4 = path.replace(expression_b1, expression_b4)\n",
        "    pathb6 = path.replace(expression_b1, expression_b6)\n",
        "\n",
        "    #creating file NDSI\n",
        "    !gdal_calc.py \\\n",
        "      --overwrite \\\n",
        "      --type=Float32 \\\n",
        "      -A {pathb4} \\\n",
        "      --A_band 1 \\\n",
        "      -B {pathb6} \\\n",
        "      --B_band 1 \\\n",
        "      --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "      --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "    !gdal_calc.py \\\n",
        "      --overwrite \\\n",
        "      --type=Float32 \\\n",
        "      --NoDataValue=0 \\\n",
        "      -A {pathb2} \\\n",
        "      --A_band 1 \\\n",
        "      -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "      --B_band 1 \\\n",
        "      --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "      --calc=\"(A.astype(float)/10000>0.11)*(B.astype(float)>=0.4)\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#\n",
        "\n",
        "    pathout = temp_dir+str('BothCheck_result.tif')\n",
        "    images.append(normalize(tif2array(pathout,0)[0]))\n",
        "    !rm -r {temp_dir}\"NDSI_result.tif\"\n",
        "    !rm -r {temp_dir}\"BothCheck_result.tif\"\n",
        "    output.clear()\n",
        "  return images\n",
        "\n",
        "\n",
        "def normalize(arr):\n",
        "    ''' Function to normalize an input array to 0-1 '''\n",
        "    arr_min = arr.min()\n",
        "    arr_max = arr.max()\n",
        "    return (arr - arr_min) / (arr_max - arr_min)\n",
        "\n",
        "\n",
        "import jax.numpy as jnp\n",
        "def batchedimages(images_path, image_list, batch_idx):\n",
        "  images = []\n",
        "  for id in range(len(batch_idx)):\n",
        "    path = os.path.join(images_path, image_list[id])\n",
        "    v1 = normalize(tif2array(path.replace(expression_b1, expression_b1),0)[0])\n",
        "    v2 = jnp.append(v1, normalize(tif2array(path.replace(expression_b1, expression_b2),0)[0]) , axis =2)\n",
        "    v3 = jnp.append(v2, normalize(tif2array(path.replace(expression_b1, expression_b3),0)[0]) , axis =2)\n",
        "    v4 = jnp.append(v3, normalize(tif2array(path.replace(expression_b1, expression_b4),0)[0]) , axis =2)\n",
        "    v5 = jnp.append(v4, normalize(tif2array(path.replace(expression_b1, expression_b5),0)[0]) , axis =2)\n",
        "    v6 = jnp.append(v5, normalize(tif2array(path.replace(expression_b1, expression_b6),0)[0]) , axis =2)\n",
        "    v7 = jnp.append(v6, normalize(tif2array(path.replace(expression_b1, expression_b7),0)[0]) , axis =2)\n",
        "    images.append(v7)\n",
        "    w1 = tif2array(path.replace(expression_b1, expression_b1),0)[0]\n",
        "    w2 = tif2array(path.replace(expression_b1, expression_b2),0)[0]\n",
        "    w3 = tif2array(path.replace(expression_b1, expression_b3),0)[0]\n",
        "    w4 = tif2array(path.replace(expression_b1, expression_b4),0)[0]\n",
        "    w5 = tif2array(path.replace(expression_b1, expression_b5),0)[0]\n",
        "    w6 = tif2array(path.replace(expression_b1, expression_b6),0)[0]\n",
        "    w7 = tif2array(path.replace(expression_b1, expression_b7),0)[0]\n",
        "  return images\n",
        "\n",
        "import jax.random as random\n",
        "import jax.numpy as jnp\n",
        "batch_size = 5\n",
        "no_of_batches = int(len(imgs_list_b1)/batch_size)\n",
        "def data_stream():\n",
        "  key = random.PRNGKey(0)\n",
        "  perm = random.permutation(key, len(imgs_list_b1))\n",
        "  x_img_list = []\n",
        "  y_img_list = []\n",
        "  for i in range(no_of_batches):\n",
        "    batch_idx = perm[i * batch_size : (i + 1) * batch_size]; #print(batch_idx)\n",
        "    # x_img_list.append(batchedimages(x_total_images_path, batch_idx))\n",
        "    # y_img_list.append(batchedimages(y_total_images_path, batch_idx))\n",
        "    yield jnp.asarray(batchedimages(image_dir, imgs_list_b1, batch_idx)), jnp.asarray(ybatchedimages(image_dir, imgs_list_b1, batch_idx))"
      ],
      "metadata": {
        "id": "_y1J6gcmLoHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RUN 2 testing**"
      ],
      "metadata": {
        "id": "OOUqoXxBUCL2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjgCYS3gUCL3"
      },
      "source": [
        "**Model and training code**\n",
        "Our model is a coordinate-based multilayer perceptron. In this example, for each input image coordinate $(x,y)$, the model predicts the associated color $(r,g,b)$ or any $(gray)$.\n",
        "\n",
        "![Network diagram](https://user-images.githubusercontent.com/3310961/85066930-ad444580-b164-11ea-9cc0-17494679e71f.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POSITIONAL ENCODING BLOCK**"
      ],
      "metadata": {
        "id": "iIR4yJ1DUCL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(args):\n",
        "    image_height_x_image_width, cha = args.shape\n",
        "    inputs_freq = jax.vmap(lambda x: args * 2.0 ** x)(jnp.arange(positional_encoding_dims))\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)])\n",
        "    x = x.swapaxes(0, 2)\n",
        "    x = x.reshape([image_height_x_image_width, -1])\n",
        "    x = jnp.concatenate([args, x], axis=-1)\n",
        "    return x\n",
        "\n",
        "def batch_encoded(args):\n",
        "    img_list = []\n",
        "    for i in range(args.shape[0]):\n",
        "        c = args[i]\n",
        "        c = c.reshape(-1, c.shape[2])\n",
        "        p = positional_encoding(c)\n",
        "        img_list.append(p.reshape(args.shape[1],args.shape[2],p.shape[1]))\n",
        "        x = jnp.array(img_list)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "A9QJZJ50UCL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP MODEL DEFINATION**\n",
        "Basically, passing input points through a simple Fourier Feature Mapping enables an MLP to learn high-frequency functions (such as an RGB image) in low-dimensional problem domains (such as a 2D coordinate of pixels)."
      ],
      "metadata": {
        "id": "AicbTZ7jUCL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "!python -m pip install -qq -U flax orbax\n",
        "# Orbax needs to enable asyncio in a Colab environment.\n",
        "!python -m pip install -qq nest_asyncio\n",
        "\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import flax\n",
        "import optax\n",
        "from typing import Any\n",
        "\n",
        "from jax import lax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state, common_utils\n",
        "\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "ndl = 8 # num_dense_layers Number of dense layers in MLP\n",
        "dlw = 256 # dense_layer_width Dimentionality of dense layers' output space\n",
        "\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "      x = batch_encoded(input_points) if self.apply_positional_encoding else input_points\n",
        "      for i in range(ndl):\n",
        "          x = nn.Dense(dlw,dtype=self.dtype,precision=self.precision)(x)\n",
        "          x = nn.relu(x)\n",
        "          x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "      x = nn.Dense(1, dtype=self.dtype, precision=self.precision)(x)\n",
        "      return x\n",
        "##########################################<< MLP MODEL >>#########################################"
      ],
      "metadata": {
        "id": "qLuo6LGHUCL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**initialize the module**"
      ],
      "metadata": {
        "id": "xGaXMyhpUCL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "!python -m pip install -q -U flax\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "\n",
        "def Create_train_state(r_key, model, shape, learning_rate ) -> train_state.TrainState:\n",
        "    print(shape)\n",
        "    variables = model.init(r_key, jnp.ones(shape))\n",
        "    optimizer = optax.adam(learning_rate)\n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        tx=optimizer,\n",
        "        params=variables['params']\n",
        "    )\n",
        "\n",
        "learning_rate = 1e-1\n",
        "model = MLPModel() # Instantiate the Model"
      ],
      "metadata": {
        "id": "FMRDDW0JUCL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**kernel visualization of the model being used**"
      ],
      "metadata": {
        "id": "zNqEeq2h2j9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def VisualizeKernel(state_parameters, epochs):\n",
        "  import cv2\n",
        "  from google.colab.patches import cv2_imshow\n",
        "  import numpy as np\n",
        "  params = state_parameters\n",
        "  a = []\n",
        "  for i in params:\n",
        "    kernel = params[str(i)]['kernel']\n",
        "    a.append(np.array(kernel))\n",
        "\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  rangex = len(params.keys())\n",
        "\n",
        "  fig, axs = plt.subplots(2, int(rangex/2)+1, figsize=(20,20))\n",
        "  images = [params[str(i)]['kernel'] for i in params]\n",
        "  for i, ax in enumerate(axs.flatten()):\n",
        "      if i < len(images):\n",
        "          ax.imshow(images[i], cmap='ocean')\n",
        "      else:\n",
        "          ax.remove()\n",
        "  plt.savefig(str(epochs)+'.png', dpi=500)\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "U_aRvrtSD1Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**defining loss function**"
      ],
      "metadata": {
        "id": "z1SX4YdxUCL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#serial\n",
        "def image_difference_loss(logits, labels):\n",
        "    loss =  0.5 * jnp.mean((logits - labels) ** 2)\n",
        "    return loss\n",
        "def compute_metrics(*, logits, labels):\n",
        "  loss = image_difference_loss(logits, labels)\n",
        "  metrics = {\n",
        "      'loss': loss,     #LOSS\n",
        "      'logits': logits, #PREDICTED IMAGE\n",
        "      'labels': labels  #ACTUAL IMAGE\n",
        "  }\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "iYslN8Z7UCL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train step defination**"
      ],
      "metadata": {
        "id": "aRW6GB7RUCL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cpu serial\n",
        "import jax\n",
        "\n",
        "def train_step(state: train_state.TrainState, batch: jnp.asarray, rng):\n",
        "    image, label = batch\n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({'params': params}, image);\n",
        "        loss =  image_difference_loss(logits, label);\n",
        "        return loss, logits\n",
        "\n",
        "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (_, logits), grads = gradient_fn(state.params)\n",
        "    new_state = state.apply_gradients(grads=grads)\n",
        "    logs = compute_metrics(logits=logits, labels=label)\n",
        "    return new_state, logs\n",
        "\n",
        "import jax\n",
        "@jax.jit\n",
        "def eval_step(state, image):\n",
        "    logits = state.apply_fn({'params': state.params}, image)\n",
        "    return compute_metrics(logits=logits, labels=image)\n"
      ],
      "metadata": {
        "id": "KUYaqngCUCL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**image viewing**"
      ],
      "metadata": {
        "id": "2xehIUuymlV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "def show_image(argu):\n",
        "  L1 = argu[0]\n",
        "  predicted_image = np.array(argu[0]*255,  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "  a = predicted_image\n",
        "  for i in range(0,argu.shape[0]):\n",
        "    predicted_image = np.array(argu[i]*255,  dtype=np.uint8).reshape(newsize)\n",
        "    a = cv2.hconcat([a, predicted_image])\n",
        "  cv2_imshow(a)"
      ],
      "metadata": {
        "id": "o_giLLzemoCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # **HIGH HEELS RUN >>>>>>>>>>>** { vertical-output: true }\n",
        "newsize = (233, 454) #(260, 260) # /.... 233 * 454\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from google.colab import output\n",
        "import orbax.checkpoint as orbax\n",
        "from flax.training import checkpoints\n",
        "\n",
        "import optax\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "CKPT_DIR = 'ckpts'\n",
        "\n",
        "######################<<<< initiating train state\n",
        "count = 0\n",
        "if count == 0 :\n",
        "  batches = data_stream()\n",
        "  BATCH, H, W, Channels = next(batches)[0].shape\n",
        "  state = Create_train_state( rng, model, (BATCH, H, W, Channels ), learning_rate )\n",
        "  count = 1\n",
        "# state = flax.jax_utils.replicate(state)  # FLAX will replicate the state to every device so that updating can be made easy\n",
        "\n",
        "######################\n",
        "checkpoint_available = 0\n",
        "pattern = re.compile(\"checkpoint_\\d+\")   # to search for \"checkpoint_*munerical value*\" numerical value of any length is denoted by regular expression \"\\d+\"\n",
        "dir = \"/content/ckpts/\"\n",
        "isFile = os.path.isdir(dir)\n",
        "if isFile:\n",
        "  for filepath in os.listdir(dir):\n",
        "      if pattern.match(filepath):\n",
        "          checkpoint_available = 1\n",
        "\n",
        "total_epochs = 50\n",
        "batches = data_stream()\n",
        "for epochs in tqdm(range(no_of_batches-5)):\n",
        "\n",
        "\n",
        "  if checkpoint_available:\n",
        "    state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "    checkpoint_available = 0 # << Flag updated >>> to stop loading the same checkpoint in the next iteration then remove the checkpoint directory\n",
        "    !rm -r {dir}\n",
        "\n",
        "  input_data = next(batches)\n",
        "\n",
        "  for bbb in tqdm(range(total_epochs)):\n",
        "    state, metrics = train_step(state, input_data, rng)\n",
        "    show_image(metrics['logits'])\n",
        "\n",
        "    # output.clear()\n",
        "    print(\"loss: \",metrics['loss'],\" <<< \") # naming of the checkpoint is \"checkpoint_*\"  where \"*\" => value of the steps variable, i.e. 'epochs'\n",
        "    if (bbb == total_epochs-1) :\n",
        "      VisualizeKernel(state.params, epochs)\n",
        "\n",
        "  orbax_checkpointer = orbax.Checkpointer(orbax.PyTreeCheckpointHandler())\n",
        "  checkpoints.save_checkpoint(ckpt_dir=CKPT_DIR, target=state, step=epochs, prefix='checkpoint_', keep=1, overwrite=False, orbax_checkpointer=orbax_checkpointer)\n",
        "  # restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state) # using to get the checkpoint loaded , it can be latest one , or if already available as checkpoint in the \"CKPT_DIR\" directory then take the file from directory then save in >> restored_checkpoints\n",
        "  ##################################################\n",
        "\n"
      ],
      "metadata": {
        "id": "rs2HKO3NUCL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**inference engine**"
      ],
      "metadata": {
        "id": "yZwxFsKYUCL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # newsize = (140,140) #(260, 260) # /.... 233 * 454\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import numpy as np\n",
        "# from google.colab import output\n",
        "\n",
        "# !wget https://live.staticflickr.com/7492/15677707699_d9d67acf9d_b.jpg -O a.jpg\n",
        "# image_in = '/content/a.jpg'\n",
        "\n",
        "# from PIL import Image\n",
        "# import jax.numpy as jnp\n",
        "# def imageRGB(argv):\n",
        "#     im = Image.open(argv)\n",
        "#     tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "#     return tvt, tvu\n",
        "# image = jnp.asarray((imageRGB(image_in)[1]))\n",
        "# #restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "# #state = restored_state\n",
        "# prediction = eval_step(state, image)\n",
        "# prediction['loss']\n",
        "\n",
        "\n",
        "# predicted_image = np.array(prediction['logits'],  dtype=np.uint8).reshape(newsize)\n",
        "# cv2_imshow(predicted_image)\n"
      ],
      "metadata": {
        "id": "u4qN8DkhUCL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}